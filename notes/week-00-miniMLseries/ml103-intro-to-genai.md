# ML103 - Introduction to Generative AI

*A lecture from the Machine Learning Mini Series by Rola Dali.  
The full series is available on YouTube: [Machine Learning Mini Series](https://youtube.com/playlist?list=PLBOxI5MJQLFs8-8vl_nTRte-gkNQeWsOa&si=3IaNr4OZUnaI9MNt).  
GitHub Repo: [miniMLseries](https://github.com/rdali/miniMLseries)*


## ğŸ“Œ Table of Contents

- [1. What is Generative AI?](#1ï¸âƒ£-what-is-generative-ai)
- [2. Large Language Models (LLMs)](#2ï¸âƒ£-large-language-models-llms)
- [3. Challenges & Limitations of LLMs](#3ï¸âƒ£-challenges--limitations-of-llms)
- [4. Key AI & LLM Terminology](#4ï¸âƒ£-key-ai--llm-terminology)
- [5. Evolution & Scale of LLMs](#5ï¸âƒ£-evolution--scale-of-llms)
- [6. Generative AI vs. Predictive ML](#6ï¸âƒ£-generative-ai-vs-predictive-ml)
- [7. Enhancing LLM Performance](#7ï¸âƒ£-enhancing-llm-performance)
- [8. Tokenization & Word Representations](#8ï¸âƒ£-tokenization--word-representations)
- [9. Transformer Architecture](#9ï¸âƒ£-transformer-architecture)
- [10. The Future of Generative AI](#-the-future-of-generative-ai)

---

## 1ï¸âƒ£ What is Generative AI?

Generative AI (**GenAI**) is a subset of **machine learning (ML)** that creates **new** content, such as text, images, audio, or video.  
Unlike traditional ML models that classify or predict within predefined constraints, **GenAI produces novel outputs** based on learned data.

### ğŸ”¹ Key Milestone

GenAI became widely known after **OpenAI released ChatGPT (Nov 2022)**, making advanced AI accessible to the public.

### ğŸ”¹ GenAI Modalities

- **Text** â†’ Large Language Models (LLMs) (e.g., GPT-4, Claude, Bard)
- **Images** â†’ AI-generated art (e.g., Midjourney, Stable Diffusion, DALLÂ·E)
- **Audio** â†’ AI music/speech synthesis (e.g., ElevenLabs, AudioCraft)
- **Video** â†’ AI video generation (e.g., Sora by OpenAI, Runway)

---

## 2ï¸âƒ£ Large Language Models (LLMs)

LLMs are **deep-learning models** designed to **process and generate human-like text**. They function as **next-word predictors**, estimating the probability of the next token in a sequence.

### ğŸ”¹ Key Features

- Trained on massive datasets including books, websites, and code.
- Understands syntax (word structure) and semantics (meaning).
- Performs a wide range of NLP tasks such as summarization, translation, and Q&A.
- Uses self-supervised learning, removing the need for labeled training data.

### ğŸ”¹ LLM Use Cases

- ğŸ“ **Text Generation** â†’ AI writers, news summarization (e.g., Jasper AI, Copy.ai)
- ğŸ’¬ **Chatbots & Conversational AI** â†’ Customer support, AI assistants (e.g., ChatGPT, Bard)
- ğŸŒ **Machine Translation** â†’ Google Translate, DeepL
- ğŸ¤– **Code Assistance** â†’ GitHub Copilot, ChatGPT Code Interpreter
- ğŸ” **Search & Retrieval** â†’ AI-enhanced search engines (e.g., Google Search with BERT)

---

## 3ï¸âƒ£ Challenges & Limitations of LLMs

âŒ **Hallucinations** â†’ LLMs may generate **plausible but incorrect information**.  
âŒ **Knowledge Cutoffs** â†’ Models only "know" data up to their last training update.  
âŒ **Bias & Toxicity** â†’ Reflects biases present in training data.  
âŒ **Context Window Limits** â†’ LLMs process a **fixed number of tokens at a time**.  
âŒ **Weak at Math & Structured Data** â†’ Struggles with calculations, tables, and logic.

---

## 4ï¸âƒ£ Key AI & LLM Terminology

- **Transformer** â†’ The neural network architecture behind LLMs (*"Attention Is All You Need"*, 2017).  
- **Token** â†’ A unit of input/output (word, subword, or character).  
- **Embedding** â†’ A numerical vector representation of text for AI processing.  
- **Retrieval-Augmented Generation (RAG)** â†’ Uses external knowledge sources to improve accuracy.  
- **Foundational Model** â†’ A large, general-purpose AI model trained on vast datasets.  

---

## 5ï¸âƒ£ Evolution & Scale of LLMs

- **LLM sizes have grown exponentially** (e.g., BERT **340M** â†’ GPT-4 **1.8T** parameters).  
- Training requires **huge compute power** (e.g., **16,000+ NVIDIA H100 GPUs, costing $65Mâ€“$85M** per model).  
- AI efficiency improvements focus on **reducing compute costs & memory requirements**.

---

## 6ï¸âƒ£ Generative AI vs. Predictive ML

| Feature            | Predictive ML         | Generative AI (LLMs)  |
|-------------------|---------------------|----------------------|
| **Training Data**  | Uses labeled data   | Uses massive unlabeled datasets |
| **Output**        | Fixed categories/numbers | Open-ended responses |
| **Use Cases**     | Fraud detection, classification | Chatbots, text/image generation |
| **Cost**         | Lower training & inference cost | Expensive training but reusable models |
| **Customization** | Often trained per use case | General-purpose but can be fine-tuned |

---

## 7ï¸âƒ£ Enhancing LLM Performance

### ğŸ”¹ Weight-Preserving Techniques (No Model Retraining)

- **Prompt Engineering** â€“ Optimizing input prompts for better responses.
- **Retrieval-Augmented Generation (RAG)** â€“ Incorporating external knowledge bases.
- **Fine-Tuning & Adaptation** â€“ Adjusting model responses with minimal data.

### ğŸ”¹ Weight-Altering Techniques (Modifying the Model Itself)

- **Parameter-Efficient Fine-Tuning (PEFT)** â€“ Efficient small-scale tuning.
- **Quantization** â€“ Reducing model size for faster inference.

---

## 8ï¸âƒ£ Tokenization & Word Representations

- **Tokenization:** Splits text into smaller pieces (*tokens*), affecting how LLMs process data.  
- **Embedding:** Converts text into mathematical representations (vectors).  
  - Early methods: **Bag of Words, TF-IDF**  
  - **Breakthrough (2013):** *Word2Vec* enabled AI to **"understand" word relationships (e.g., King - Man + Woman = Queen).**  
  - **Transformers (2017):** Enabled **deep contextual understanding** of text.  

---

## 9ï¸âƒ£ Transformer Architecture

The **transformer** is the neural network architecture that powers **LLMs**.

### ğŸ”¹ Key Components

- **Self-Attention Mechanism** â†’ Assigns importance to words in a sentence.  
- **Multi-Head Attention** â†’ Tracks multiple contextual relationships.  
- **Positional Encoding** â†’ Retains word order in parallel processing.  

### ğŸ”¹ Why Transformers Replaced RNNs

âœ”ï¸ **Parallelization** â†’ Faster training & inference.  
âœ”ï¸ **Better handling of long-range dependencies** in text.  

---

## ğŸ”Ÿ The Future of Generative AI

- **LLMs will continue scaling**, with model sizes increasing by roughly 10x per year.
- **Optimization & efficiency improvements** will drive more sustainable AI training.
- **Greater AI accessibility**, with more open-source models becoming available.
- **Emergence of new architectures** beyond transformers for faster, more efficient AI.

---

## Summary

âœ”ï¸ **Generative AI enables machines to generate new, human-like content.**  
âœ”ï¸ **LLMs are transformer-based AI models used in chatbots, coding, translation, and more.**  
âœ”ï¸ **LLMs have limitations (hallucinations, bias, high compute costs).**  
âœ”ï¸ **Techniques like RAG, fine-tuning, and prompt engineering improve LLM performance.**  
âœ”ï¸ **The future of AI will focus on scalability, optimization, and accessibility.**  

---
*Made by Ramsi K. â€“ Part of the GenAI Bootcamp 2025 repository.*
