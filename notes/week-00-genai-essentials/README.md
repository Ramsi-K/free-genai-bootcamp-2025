# Week 00: GenAI Essentials  

## Overview  
This week covers the **fundamentals of AI and Generative AI**, focusing on:  
- The basics of **AI, ML, and DL**  
- Understanding **Generative AI vs. Traditional AI**  
- The **transformer architecture and tokenization**  
- How **LLMs are trained, fine-tuned, and deployed**  
- Hands-on **labs for prompt engineering and model evaluation**  

---

## ðŸ“‚ Notes for This Week  

| **Topic** | **Description** |
|-----------|----------------|
| [**Intro to AI & Generative AI**](01-introduction-to-ai.md) | Covers AI, ML, DL, Generative AI, and LLMs. |
| [**Transformer Architecture**](02-transformer-architecture.md) | Breakdown of the transformer model, attention mechanisms, and tokenization. |
| [**Fine-Tuning LLMs**](03-fine-tuning-llms.md) | Techniques like supervised fine-tuning, LoRA, RLHF. |
| [**Data & Machine Learning**](04-data-and-ml.md) | Data mining, wrangling, labeling, and ML pipelines. |
| [**Prompt Engineering**](05-prompt-engineering.md) | Strategies like Few-shot, Chain-of-Thought, ReAct. |
| [**LLM Development Tools**](06-llm-dev-tools.md) | Tools like Hugging Face, LangChain, LlamaIndex. |
| [**Model as a Service**](07-model-as-a-service.md) | Overview of Amazon Bedrock, Azure AI, Google Vertex AI. |
| [**GenAI Security**](08-genai-security.md) | AI security risks like prompt injection, adversarial attacks. |
| [**Context Caching**](09-context-caching.md) | Optimizing model efficiency with caching techniques. |
| [**GenAI Hardware**](10-hardware-for-genai.md) | GPUs, TPUs, Intel Gaudi, cloud infrastructure. |
| [**Evaluations & Metrics**](11-evaluations-and-metrics.md) | Perplexity, BLEU scores, token efficiency benchmarks. |
| [**Serving Models**](12-serving-models.md) | KServe, Ray, TensorRT LLM, and real-time deployment. |
| [**Enterprise & Production AI**](13-production-enterprise.md) | Scaling AI models, multi-cloud deployment, cost optimization. |

---

## ðŸ”¥ Key Learnings  
- **AI vs. Generative AI**: Key differences in functionality and output.  
- **How transformers revolutionized NLP** through self-attention.  
- **Optimizing AI efficiency** with caching, quantization, and better architectures.  
- **Security risks in AI** and how to mitigate them.  

---

This covers the **foundations** of the bootcamp.
