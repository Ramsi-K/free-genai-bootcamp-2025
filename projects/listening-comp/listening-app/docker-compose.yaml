services:
  base:
    build:
      context: .
      dockerfile: Dockerfile.base
    image: listening-app-base:latest
    command: tail -f /dev/null
    networks:
      - app_network

  main:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - '8000:8000'
    environment:
      - HOST_IP=${HOST_IP}
      - LLM_MODEL=${LLM_MODEL}
      - USE_GPU=${USE_GPU}
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    depends_on:
      - otel-collector
      - prometheus
      - ollama
    networks:
      - app_network
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8000/health']
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G # Limit to 1GB of RAM
          cpus: '1.00' # Limit to 1 CPU core

  transcript-processor:
    build:
      context: ./services/transcript-processor
      dockerfile: Dockerfile
    ports:
      - '5000:5000'
    environment:
      - DB_PATH=/shared/data/app.db
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    depends_on:
      - otel-collector
    volumes:
      - shared_data:/shared/data
    networks:
      - app_network
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5000/health']
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M # Limit to 512MB of RAM
          cpus: '0.50' # Limit to 50% of a single CPU core

  question-module:
    build:
      context: ./services/question-module
      dockerfile: Dockerfile
    ports:
      - '5001:5001'
    environment:
      - DB_PATH=/shared/data/app.db
      - LLM_MODEL=${LLM_MODEL}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    depends_on:
      - otel-collector
    volumes:
      - shared_data:/shared/data
    networks:
      - app_network
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5001/health']
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M # Limit to 512MB of RAM
          cpus: '0.50' # Limit to 50% of a single CPU core

  audio-module:
    build:
      context: ./services
      dockerfile: audio-module/Dockerfile
    ports:
      - '5002:5002'
    environment:
      - DB_PATH=/shared/data/app.db
      - USE_GPU=${USE_GPU}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    depends_on:
      - otel-collector
      - melotts
    volumes:
      - shared_data:/shared/data
    networks:
      - app_network
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5002/health']
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M # Limit to 512MB of RAM
          cpus: '0.50' # Limit to 50% of a single CPU core

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: korean-frontend
    ports:
      - '80:80'
    environment:
      - REACT_APP_API_BASE_URL=http://${HOST_IP:-localhost}:5000
      - REACT_APP_QUESTION_API_URL=http://${HOST_IP:-localhost}:5001
      - REACT_APP_AUDIO_API_URL=http://${HOST_IP:-localhost}:5002
    depends_on:
      - transcript-processor
      - question-module
      - audio-module
    networks:
      - app_network
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - '8008:11434'
    volumes:
      - ollama_data:/root/.ollama
    command: serve
    networks:
      - app_network
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:11434/api/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # ollama-server:
  #   image: ollama/ollama
  #   container_name: ollama-server
  #   ports:
  #     - '11434:11434'
  #   environment:
  #     - no_proxy=${NO_PROXY}
  #     - http_proxy=${HTTP_PROXY}
  #     - https_proxy=${HTTPS_PROXY}
  #     - LLM_MODEL=${LLM_MODEL}
  #   volumes:
  #     - ollama_data:/root/.ollama
  #     - ./start.sh:/start.sh
  #   entrypoint: ['/bin/bash', '/start.sh']
  #   networks:
  #     - app_network

  otel-collector:
    image: otel/opentelemetry-collector:latest
    volumes:
      - ./otel-config.yaml:/etc/otel-config.yaml
    command: ['--config=/etc/otel-config.yaml']
    ports:
      - '4317:4317'
      - '4318:4318' # Added HTTP endpoint
    networks:
      - app_network
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - '9090:9090'
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    networks:
      - app_network
    restart: unless-stopped

  melotts:
    build:
      context: ./MeloTTS
      dockerfile: Dockerfile
    ports:
      - '8888:8888'
    networks:
      - app_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.00'
    environment:
      - USE_GPU=${USE_GPU}

volumes:
  shared_data:
  ollama_data:
    driver: local
  prometheus_data:

networks:
  app_network:
    driver: bridge
