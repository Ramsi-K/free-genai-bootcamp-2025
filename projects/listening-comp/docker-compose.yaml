services:
  # Transcript Processor Service
  transcript-processor:
    build:
      context: .
      dockerfile: services/transcript-processor/Dockerfile
    ports:
      - '5000:5000'
    volumes:
      - shared_data:/shared/data
      - ./services/transcript-processor:/app/src
    environment:
      - OUTPUT_DIR=/shared/data
      - USE_GPU=${USE_GPU}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - PYTHONUNBUFFERED=1
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Question Module Service
  question-module:
    build:
      context: .
      dockerfile: services/question-module/Dockerfile
    ports:
      - '5001:5001'
    volumes:
      - shared_data:/shared/data
      - ./services/question-module:/app/src
    environment:
      - DATA_DIR=/shared/data
      - CHROMA_DIR=/shared/data/chroma
      - OLLAMA_HOST=http://ollama:11434
      - USE_GPU=${USE_GPU}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - LLM_MODEL=${LLM_MODEL}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - PYTHONUNBUFFERED=1
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Audio Module Service
  audio-module:
    build:
      context: .
      dockerfile: services/audio-module/Dockerfile
    ports:
      - '5002:5002'
    volumes:
      - shared_data:/shared/data
      - ./services/audio-module:/app/src
    environment:
      - DATA_DIR=/shared/data
      - AUDIO_DIR=/shared/data/audio
      - USE_GPU=${USE_GPU}
      - TTS_MODEL=${TTS_MODEL}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - PYTHONUNBUFFERED=1
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Frontend React Application
  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: korean-frontend
    ports:
      - '80:80'
    environment:
      - REACT_APP_API_BASE_URL=http://${HOST_IP:-localhost}:5000
      - REACT_APP_QUESTION_API_URL=http://${HOST_IP:-localhost}:5001
      - REACT_APP_AUDIO_API_URL=http://${HOST_IP:-localhost}:5002
    depends_on:
      - transcript-processor
      - question-module
      - audio-module
    restart: unless-stopped

  # Ollama LLM Service
  ollama:
    image: ollama/ollama:latest
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
    command:
      [
        'sh',
        '-c',
        'ollama serve & sleep 10 && ollama pull ${LLM_MODEL} && tail -f /dev/null',
      ]

  # Add mega-service
  mega-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - '8000:8000'
    volumes:
      - shared_data:/shared/data
    environment:
      - OLLAMA_HOST=ollama
    depends_on:
      - transcript-processor
      - question-module
      - audio-module
      - ollama

  prometheus:
    image: prom/prometheus:latest
    ports:
      - '9090:9090'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--storage.tsdb.retention.time=90d' # Keep data for 90 days
      - '--storage.tsdb.path=/prometheus'
    depends_on:
      - mega-service
      - transcript-processor
      - question-module
      - audio-module

  grafana:
    image: grafana/grafana:latest
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - '16686:16686'
      - '14250:14250'
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - elasticsearch

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    environment:
      - discovery.type=single-node
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  otel-collector:
    image: otel/opentelemetry-collector:latest
    command: ['--config=/etc/otel-collector-config.yaml']
    volumes:
      - ./otel-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - '4317:4317'
      - '4318:4318'
    depends_on:
      - jaeger

volumes:
  shared_data:
  ollama_data:
  grafana_data:
  prometheus_data:
  elasticsearch_data:
