version: '3.8'

services:
  # Transcript Processor Service
  transcript-processor:
    build:
      context: ./services/transcript-processor
      dockerfile: Dockerfile
    container_name: korean-transcript-processor
    ports:
      - "5000:5000"
    volumes:
      - shared-data:/shared/data
    environment:
      - OUTPUT_DIR=/shared/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Question Module Service
  question-module:
    build:
      context: ./services/question-module
      dockerfile: Dockerfile
    container_name: korean-question-module
    ports:
      - "5001:5001"
    volumes:
      - shared-data:/shared/data
      - chroma-db:/shared/data/chroma
    environment:
      - DATA_DIR=/shared/data
      - CHROMA_DIR=/shared/data/chroma
      - OLLAMA_HOST=http://ollama:11434
      - LLM_MODEL=llama3:8b
      - TEI_HOST=http://tei-embedding-service
      - TEI_PORT=80
    depends_on:
      - transcript-processor
      - ollama
      - tei-embedding-service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Audio Module Service
  audio-module:
    build:
      context: ./services/audio-module
      dockerfile: Dockerfile
    container_name: korean-audio-module
    ports:
      - "5002:5002"
    volumes:
      - shared-data:/shared/data
    environment:
      - DATA_DIR=/shared/data
      - AUDIO_DIR=/shared/data/audio
      - USE_GPU=true
    depends_on:
      - transcript-processor
      - question-module
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Frontend React Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: korean-frontend
    ports:
      - "80:80"
    environment:
      - REACT_APP_API_BASE_URL=http://${HOST_IP:-localhost}:5000
      - REACT_APP_QUESTION_API_URL=http://${HOST_IP:-localhost}:5001
      - REACT_APP_AUDIO_API_URL=http://${HOST_IP:-localhost}:5002
    depends_on:
      - transcript-processor
      - question-module
      - audio-module
    restart: unless-stopped

  # Ollama LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: korean-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    command: ["serve"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # TEI Embedding Service
  tei-embedding-service:
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    container_name: korean-tei-embedding
    ports:
      - "8090:80"
    volumes:
      - tei-data:/data
    environment:
      - HF_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
    command: --model-id jhgan/ko-sroberta-multitask
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  shared-data:
  chroma-db:
  ollama-data:
  tei-data:
